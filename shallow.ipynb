{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "Python 3.6.8 64-bit",
      "display_name": "Python 3.6.8 64-bit",
      "metadata": {
        "interpreter": {
          "hash": "ffa8138a954375a0eba8eca80543292cc4faeae39ef0340fcb1267261a1ca77f"
        }
      }
    },
    "colab": {
      "name": "shallow.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hikmatfarhat-ndu/CSC645/blob/master/shallow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHNba3P7AjEZ"
      },
      "source": [
        "# A shallow (two layers) network- Recognizing Sonar data\n",
        "In this exercises we will use a **two layer** (1 input, 1 hidden and 1 output) neural network to classify a two-class **sonar** data. Each entry is the result of bouncing off sonar signal from different angles at metals cylinder (Mines) and rock (Rock) objects. It contains 60 values between 0 and 1 and a corresponding label (M or R). A detailed description of the data set can be found [here](https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks))\n",
        "\n",
        "Go ahead and download to your computer the file sonar.all-data from  [here](https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/)\n",
        "and **rename** the file sonar.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZB1NCGg_AjEd"
      },
      "source": [
        "### Importing packages\n",
        "We need the follwing packages: numpy for the computation, google.colab for loading the data file into the colab notebook and finally pandas for reading the data from the file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MFSVd9VAjEg"
      },
      "source": [
        "import numpy as np\n",
        "#from google.colab import files\n",
        "import pandas as pd"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWygj19pAjE1"
      },
      "source": [
        "### Reading the data\n",
        "Upload the data file to colab and read it using the pandas package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXRfaXGsqtjA",
        "outputId": "64fd79ed-f977-4ee9-88dc-33097073810a",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "#file=files.upload()\n",
        "df=pd.read_csv(\"sonar.csv\")"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdtWZ0JK0nEo"
      },
      "source": [
        "## Preprocessing the data\n",
        "\n",
        "Before we start the learning process we need to preprocess the data. First, all the Mines \"M\" are grouped together and the Rocks \"R\" are grouped together. We use the numpy shuffle function to mix them randomly. Second, Pandas reads the data as pandas frame so we need to extract the data values and the label values. Third, we convert the labels from \"M\" to 1 and from \"R\" to 0. Finally, we divide the data set into training and test subsets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BA9AdUF3q3uH",
        "outputId": "055950b8-1d02-4edf-baed-58ab2412bba0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "tags": []
      },
      "source": [
        "#pandas data frame\n",
        "m=df.values\n",
        "# randomize (shuffle) the data\n",
        "np.random.shuffle(m)\n",
        "m=m.T\n",
        "# Each row has 61 entries, 60 for data and the last one is the label \"M\" or \"R\"\n",
        "\n",
        "# X contains all the data\n",
        "X=m[0:60,:].astype(\"float32\")\n",
        "\n",
        "# Y contains all the labels\n",
        "\n",
        "Y=m[60,:]\n",
        "# convert the labels: \"M\"->1 and \"R\"->0\n",
        "Y=np.array([1.0 if i=='M' else 0.0 for i in Y])\n",
        "Y=Y.reshape((1,len(Y)))\n",
        "Y=Y.astype(\"float32\")\n",
        "\n",
        "# split the data and labels into a training and test sets\n",
        "x_train=X[:,0:175]\n",
        "x_test=X[:,175:208]\n",
        "y_train=Y[:,0:175]\n",
        "y_test=Y[:,175:208]\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "(60, 175)\n(60, 32)\n(1, 175)\n(1, 32)\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5WG9zM11JvV"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "source": [
        "learning_rate = 3\n",
        "nb_iterations = 2000\n",
        "# Network Parameters\n",
        "n_h = 64 # number of neurons in hidden layer\n",
        "n_x = x_train.shape[0] #number of neurons in input\n",
        "n_y = y_train.shape[0] #number of neurons in ouput"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "-pZo-cdyAjFl"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-nWNb3XAjFn"
      },
      "source": [
        "![alt text](shallow-example.png \"Title\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r41rVrvQAjFp"
      },
      "source": [
        "### Sigmoid function\n",
        "First write the sigmoid function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nPwBnlHAjFq"
      },
      "source": [
        "def sigmoid(z):\n",
        "    s=1/(1+np.exp(-z))\n",
        "    return s"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDqKwr4MAjF3"
      },
      "source": [
        "### Initializing the parameters\n",
        "Since we have two layers we will need two weight matrices and two bias vectors. Consult the forward propagation equations shown below to be able to determine the shape of the parameters and therefore initialize them.\n",
        "$\\sigma$ is the sigmoid function defined above, $A^0=X$ is the input, $A^1$ and $A^2$ are the output of the first and second layers respectively. Recall that all the variables below (except the parameters) are vectorized version containing all the samples where the samples are column stacked. So X[:,0] is the input of the first (0) sample\n",
        "Z1[0,0] is the output of the first node in the first layer when the input is the first sample, etc..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f80RjsXsAjF5"
      },
      "source": [
        "\\begin{align*}\n",
        "    Z^1&=W^0\\cdot A^0+B^0\\\\\n",
        "    A^1&=\\sigma(Z^1)\\\\\n",
        "    Z^2&=W^1\\cdot A^1+B^1\\\\\n",
        "    A^2&=\\sigma(Z^2)\n",
        "  \\end{align*}\n",
        "We initialize the weights randomly and the biases to zero. This is done in numpy by using the random.randn and zeros functions. To create an nxm matrix of random numbers we use np.random.randn(n,m) and to create an nxm matrix of zeros we use np.zeros((n,m))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwngNIdsAjF7"
      },
      "source": [
        "\n",
        "W0=np.random.randn(n_h,n_x)\n",
        "b0=np.zeros((n_h,1))\n",
        "W1=np.random.randn(n_y,n_h)\n",
        "b1=np.zeros((n_y,1))\n",
        "#dW0=0.001*W0\n",
        "#db0=b0\n",
        "#dW1=0.001*W1\n",
        "#db1=b1\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss(A2,Y):\n",
        "    m=Y.shape[1]\n",
        "    logprob=Y*np.log(A2)+(1-Y)*np.log(1-A2)\n",
        "    cost=-np.sum(logprob)/m\n",
        "    cost=np.squeeze(cost)\n",
        "    return cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjIUGuanAjGS"
      },
      "source": [
        "## Forward Propagation\n",
        "To implement forward propagation recall that \n",
        "  \\begin{align*}\n",
        "    Z^1&=W^0\\cdot A^0+b^0\\\\\n",
        "    A^1&=\\sigma(Z^1)\\\\\n",
        "    Z^2&=W^1\\cdot A^1+b^1\\\\\n",
        "    A^2&=\\sigma(Z^2)\n",
        "  \\end{align*}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-OaRqQFAjGU"
      },
      "source": [
        "def model(X):\n",
        "    Z1=np.dot(W0,X)+b0\n",
        "    A1=sigmoid(Z1)\n",
        "    Z2=np.dot(W1,A1)+b1\n",
        "    A2=sigmoid(Z2)\n",
        "    \n",
        "    return A1,A2"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq1IM6c7AjGi"
      },
      "source": [
        "### Back propagation\n",
        "To compute the gradients recall the formulas from class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ei1fUOXXAjGn"
      },
      "source": [
        "\\begin{align*}\n",
        "   db^1&=\\frac{1}{m}\\sum_s(A^2-Y) & (1,1)\\\\\n",
        "      dW1&=\\frac{1}{m}(A^2-Y)\\cdot {A^1}^T& (1,m)\\times(m,n_h)=(1,n_h)\\\\\n",
        "      db^0&=\\frac{1}{m}\\sum_s\\left[{W^1}^T\\cdot (A^2-Y)\\right]*\\sigma' & \\sum_s (n_h,1)\\times (1,m)=(n_h,1)\\\\\n",
        "      dW^0&=\\frac{1}{m}\\left[\\left({W^1}^T\\cdot (A^2-Y)\\right)*\\sigma'\\right]\\cdot X^T &(n_h,1)\\times(1,m)\\times(m,2)=(n_h,2)\n",
        "    \\end{align*}\n",
        "\n",
        "\n",
        "It is convenient to add temporary variables dZ2 and dZ1 defined as: $dZ2=A^2-Y$, $dZ1=\\left({W^1}^T\\cdot dZ2\\right)*\\sigma'$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyXDrFIvAjGq"
      },
      "source": [
        "#def back_propagation(parameters,X,Y):\n",
        "def gradient(X,Y):\n",
        "    global dW0,db0,dW1,db1\n",
        "    #we will be dividing by the number of samples m\n",
        "    m=X.shape[1]\n",
        "    \n",
        "    A1,A2=model(X)\n",
        "    cost=loss(A2,Y)\n",
        "    \n",
        "    # the derivative of the sigmoid\n",
        "    gp=A1*(1-A1)\n",
        "    #we will use some temporary variables\n",
        "    dZ2=A2-Y\n",
        "    dW1=np.dot(dZ2,A1.T)/m\n",
        "    db1=np.sum(dZ2,axis=1,keepdims=True)/m\n",
        "    dZ1=np.dot(W1.T,dZ2)*gp\n",
        "    dW0=np.dot(dZ1,X.T)/m\n",
        "    db0=np.sum(dZ1,axis=1,keepdims=True)/m\n",
        "    return cost\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "0.7631966068072591"
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "gradient(x_train,y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuMzsktaAjG5"
      },
      "source": [
        "### Updating the parameters\n",
        "For every iteration we need to update the parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zQurzD7AjG6"
      },
      "source": [
        "def apply_gradients(learning_rate):\n",
        "\n",
        "    global W0,b0,W1,b1\n",
        "    W0=W0-learning_rate*dW0\n",
        "    b0=b0-learning_rate*db0\n",
        "    W1=W1-learning_rate*dW1\n",
        "    b1=b1-learning_rate*db1\n",
        "    "
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJODqTJ4AjHF"
      },
      "source": [
        "### Computing the cost\n",
        "Recall that for $m$ samples we defined the cross-entropy cost function as\n",
        "\\begin{align*}\n",
        "cost=\\frac{-1}{m}\\sum_s Y*\\log A^2+(1-Y)*\\log (1-A^2)\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofCF5L77AjHR"
      },
      "source": [
        "### Gradient Descent\n",
        "Having implemented all the above functions now we can implement gradient descent. Note that we are\n",
        "using the number of nodes in the hidden layer as a variable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9Ox-a2VAjHU",
        "tags": []
      },
      "source": [
        "for i in range(nb_iterations):\n",
        "    cost=gradient(x_train,y_train)\n",
        "    apply_gradients(learning_rate)\n",
        "    if i % 500 == 0:\n",
        "        print (\"Cost after iteration %i: %f\" %(i, cost))\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Cost after iteration 0: 0.763197\nCost after iteration 500: 0.116275\nCost after iteration 1000: 0.013728\nCost after iteration 1500: 0.006381\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iomBKV_LAjHe"
      },
      "source": [
        "### Evaluating the results\n",
        "At this point our network has learned the parameters. We test the predictions as follows: we compute the output $A^2$ and for every data point if the value of $A^2>0.5$ we predict red otherwise it is blue. After that we accumulate all the correct predictions. A prediction for data point $i$ is correct if $Y[i]=1$ and $A^2[i]=1$ or $Y[i]=0$ and \n",
        "$A^2[i]=0$. The sum of all correct predictions can be done nicely using the formula belwo\n",
        "\\begin{align*}\n",
        " Y\\cdot {A^2}^T+(1-Y)\\cdot(1-{A^2}^T)\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "q8Na7ehlAjHq",
        "outputId": "5fffb3b6-1179-4b3c-e4c3-5835db0ab9fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        }
      },
      "source": [
        "A1,A2=model(x_test)\n",
        "predictions=(A2>0.5)\n",
        "correct=np.dot(y_test,predictions.T)+np.dot(1-y_test,1-predictions.T)\n",
        "accuracy=100*float(correct)/float(y_test.shape[1])\n",
        "print(\"Accuracy=\"+str(accuracy))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Accuracy=90.625\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}
